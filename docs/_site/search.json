[
  {
    "objectID": "storage-recommend-typist.html",
    "href": "storage-recommend-typist.html",
    "title": "Posit Team Cloud Storage Recommendations",
    "section": "",
    "text": "The storage solution that supports your deployment of Posit Team is critical to achieving the level of performance and reliability that your data scientists expect and need to support the important research, development and insight generation they are striving for. The correct storage choice can ultimately save your organization time, money and frustration, by enabling a desktop like experience for your users, while providing a secure, server-sized compute environment. Because this choice can be the linchpin of a successful and satisfying deployment of Posit Workbench, Connect and Package Manager, Posit has done extensive testing and troubleshooting with a variety of cloud storage solutions. This testing was done using a combination of artificial and real-world storage loading scenarios to try and best capture the characteristics of the underlying cloud storage solution.\n\nPosit Team Relative Storage Requirements\nThe networked and local storage needs and requirements for Posit products vary between Workbench, Connect and Package Manager. Workbench represents the most storage dependent of the product suite, because it directly hosts interactive user sessions which need to be saved, cached and run as quickly as user’s desktop environments. Additionally, Workbench uses the users home directory heavily as a location for storing Workbench session state as well as suspended session data, which can grow quite large, depending the work the user is doing.\nConnects storage requirements are, in general, less demanding. This is because while Connect still needs fast, responsive storage, the majority of the storage intensive operations happen asynchronously, during the publishing process. This means that unless the application/content item is expected to do heavy read/write operations at run time, the startup and loading time for a particular application is only marginally impacted by the speed of the underlying storage.\nPackage Manager is the least storage intensive application within Posit Team. It’s primary use for storage is to locally cache packages downloaded from the Posit Sync Service and then expose those packages for consumption by your internal data science teams. This low threshhold for storage performance is further evidenced by the fact that it’s cached package data can be served from S3.\n\n\nGeneral Storage Recommendations\nThere are two components to a storage solution which will generally provide an excellent user-experience with Posit products, in particular Workbench. The two primary requirements are throughput and storage latency. Storage throughput, generally measured in MB/s, is a measure of the maximum rate at which data can be moved from server memory to disk, whether that disk is a local SSD or an NFS server share hosted in another country. Throughput is generally most utilized in data science workloads when reading or writing large data files to disk, as well as when a user session that included large amounts of data stored in memory is suspended or resumed by Workbench. Low throughput or otherwise slow storage can massively impact code run time as well as the time it takes to suspend and resume user sessions. Insufficient throughput on a remote NFS server can cause cascading impacts to other users Workbench sessions, if the network throughput of the remote NFS server where user home directories are stored is exhausted.\nThe other important metric for data science workloads is storage latency, measured in milliseconds, which represents the amount of time a single storage transaction takes to complete. Storage latency is particularly important because session state is rapidly cached to disk, to make sure that no data or inputs are lost in the event of a browser disconnect. Slowness in disk latency can cause the RStudio IDE to lag and respond slowly to user input, while the IDE is trying to confirm that user input is cached.\nIOPS, while a key metric for many storage providers and solutions is rarely the limiting factor with most cloud storage solutions, because while Posit products do read and write large files, and require the ability to quickly complete small file operations, most cloud storage providers provide sufficient baseline IOPS for SSD/Flash storage arrays.\nNow, on to the recommendations, generally we recommend using this formula to calculate your required throughput needs:\nThroughput-of-1-node + (Throughput-of-1-node*Number-of-Remaining-Nodes*.1)\nFor example, if you are running HA Workbench with 3 nodes and each node is capable of 1 Gb/s throughput, we recommend that your storage solution is able to handle 1.2 Gb/s. Using the formula:\n1 Gb/s + (1 Gb/s * 2 * .1) = 1.2 Gb/s\nThis configuration generally provides sufficient throughput to allow for a heavy workload on a single node, while the remaining nodes still have the necessary throughput to handle general interactive sessions.\nOn the latency side, generally under 1ms average latency measured by ioping provides a good user experience. Anything greater than 2-2.5ms might be perceived as slow performance, but depending on the other characteristics of the storage solution can work. Latency greater than 2.5ms for a simple ioping average causes user perceived performance slowness and can make Workbench appear unresponsive. Examples of cloud storage solutions in this last class that administrators should avoid are Azure Files (Storage account), Azure Elastic SAN, and AWS EFS Regional.\n\n\nCloud Specific Recommendations\n\nAWSAzureGCP\n\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\n\nStorage Service\nWorkbench Home Directory\nConnect Application Directory\nPackage Manager Local Cache\nProject Sharing Support\nFile locks\n\n\n\n\nEBS Local Storage\n✅\n✅\n✅\n✅\n✅\n\n\nAWS Fsx for Lustre\n✅\n✅\n✅\n✅\n✅\n\n\nAWS Fsx for NetApp ONTAP\n✅\n✅\n✅\n❌\n✅\n\n\nAWS Fsx for OpenZFS\n✅\n✅\n✅\n❌\n✅\n\n\nEFS One Zone\n⚠️\n✅\n✅\n❌\n✅\n\n\nEFS Regional\n⚠️\n✅\n✅\n❌\n✅\n\n\nS3 Bucket\n❌\n❌\n✅\n❌\n❌\n\n\n\n\nEBS Local Storage\n\nThis standard block storage offering provides, as expected, the fastest storage experience in most Posit workloads, but unfortunately doesn’t support High-Availability or Load Balanced configurations for Posit products. Additionally, backups and point in time recovery for EBS storage can result in lost data and/or work due to the generally unreplicated nature of the storage.\n\nAWS Fsx for Lustre\n\nFsx for Lustre provides a premium experience for data science users and supports the extended POSIX ACLs that Posit Workbench Project Sharing requires. Unfortunately, it does not include a multi-AZ redundancy option that is easy to configure. Fsx for Lustre’s replication configuration involves a potentially complicated process involving S3 bucket replication. Link S3 and Fsx for Lustre\n\nAWS Fsx for NetApp ONTAP and AWS Fsx for OpenZFS\n\nBoth filesystems, when correclty configured, provide excellent performance characteristics for most Posit Team use-cases. They do not support Workbench project sharing, because they lack support for extended Posix ACLs. With these filesystems, the key consideration is the synchronous nature of multi-AZ replication which can introduce significant file system latency and result in perceived slowness for data science users doing simple tasks like installing R packages and creation Python virtual environments. This latency only exists for write performance, because read operations can be returned directly from the primary ZFS server.\n\nEFS One Zone and EFS Regional\n\nEFS storage deployments are a relatively inexpensive storage solution for the Posit Team suite of products. Cost reduction is the primary benefit, because they are generally slower than the other supported options and, in the case of EFS Regional, much slower. EFS Regional should not be considered for home directory storage for Workbench, though EFS One Zone is serviceable. The speed difference between EFS One Zone and EBS/Fsx solutions will directly impact user workloads and general operations in Workbench such as installing packages and creating Python virtual environments. EFS deployments are generally performant enough for both Posit Connect and Package Manager, as long as your Connect applications can be tuned to deal with the potentially extended application startup times. Package Manager is capable of dealing with slower shared filesystems and can run from EFS perfectly, but generally S3 is a better option for Package Manager.\n\n\n\n\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\n\nStorage Service\nWorkbench Home Directory\nConnect Application Directory\nPackage Manager Local Cache\nProject Sharing Support\nFile locks\n\n\n\n\nManaged Premium SSD LRS\n✅\n✅\n✅\n✅\n✅\n\n\nAzure Netapp Files - Standard\n⚠️\n⚠️\n✅\n✅\n✅\n\n\nAzure Netapp Files - Premium\n✅\n✅\n✅\n✅\n✅\n\n\nAzure Netapp Files - Ultra\n✅\n✅\n✅\n✅\n✅\n\n\nAzure Files - Storage Acct\n❌\n⚠️\n✅\n❌\n✅\n\n\nAzure Elastic SAN\n❌\n❌\n❌\n❌\n✅\n\n\n\n\nManaged Premium Disk LRS\n\nThis standard block storage offering provides, as expected, the fastest storage experience in most Posit workloads, but unfortunately doesn’t support High-Availability or Load Balanced configurations for Posit products. Additionally, backups and point in time recovery for Managed Disk storage can result in lost data and/or work due to the generally unreplicated nature of the storage. Posit recommends SSD block storage for all Posit workloads where data access speed is the most important factor in your storage selection process.\n\nAzure Netapp Files\n\nAzure Netapp files provides an excellent experience when used as Posit storage. Posit recommends locating the Netapp storage volume used with out products in the same availability zone as your Azure VM. Azure Netapp storage throttles throughput with storage capacity and storage tier, Standard, Premium and Ultra. Posit generally recommends against Azure Netapp- Standard for use with our products, unless your organization needs or uses a lot of Azure Netapp storage, because Standard doesn’t provide much througput per TB of storage. Details can be found in Microsofts Azure Netapp article, which you can compare with our suggested throughput formula above to determine the best capacity/throughput/cost configuration for your organization.\n\nAzure Files NFS - Storage Account\n\nAzure Files initially appears to be a low-cost, scalable NFS file storage option in Azure, but unfortunately it suffers from several major drawbacks when used with Posit products and data science workloads. Firstly, it has very high file system latency, which results in a poor experience for the majority of Posit Workbench use-cases. Additionally, this high latency can result in a higher than expected occurence of orphaned ..nfs12345678 files, which can lock/abort applicaiton workloads. These leftover files are the result of the NFSv3 Silly Rename process. Additionally, Azure Files suffers from known filesystem latency issues on metadata operations which further compounds it’s unsuitability for workloads that include lots of small files, such as R and Python package installation and restoration.\n\nAzure Elastic SAN\n\nAzure Elastic SAN showed inconsistent performance in our testing and while it may be tunable to ultimately be an excellent storage solution, the complexity and effort required added to the availability of more suitable, easier to support options like Managed Disk for local storage and Azure Netapp files for networked storage, mean that it is generally not recommended, unless your organization is already using Elastic SAN and can’t use other options.\n\n\n\n\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\n\nStorage Service\nWorkbench Home Directory\nConnect Application Directory\nPackage Manager Local Cache\nProject Sharing Support\nFile locks\n\n\n\n\nSSD Persistent Disk\n✅\n✅\n✅\n✅\n✅\n\n\nGoogle File Store - Basic SSD\n✅\n✅\n✅\n✅\n✅\n\n\nGoogle File Store - Zonal SSD\n⚠️\n✅\n✅\n✅\n✅\n\n\nGoogle File Store - Enterprise SSD\n⚠️\n✅\n✅\n✅\n✅\n\n\nGoogle Cloud Storage\n❌\n❌\n❌\n❌\n❌\n\n\n\n\nSSD Persistent Disk\n\nThis standard block storage offering provides, as expected, the fastest storage experience in most Posit workloads, but unfortunately doesn’t support High-Availability or Load Balanced configurations for Posit products. Additionally, backups and point in time recovery for local disk storage can result in lost data and/or work due to the generally unreplicated nature of the storage. Posit recommends SSD block storage for all Posit workloads where data access speed is the most important factor in your storage selection process.\n\nGoogle File Store\n\nGoogle File Store is one of the core NFS file server options available in GCP and thus is the primary recommended filestore for Posit products in GCP. Unfortunately, it is slower compared to other AWS and Azure cloud storage solutions, at least for single VM workloads which was how Posit tested. Zonal and Enterprise SSD, likely due to their HA/replicated configuration they have higher latency, but higher throughput than Basic SSD. This presents as shorter small file write times for Basic SSD, which manifests as shorter R and Python package installation times, but longer large file installations due to lower throughput. Posit recommends sizing your GFS deployment to make sure that you’re able to reach a good balance between throughput and latency for your users.\n\nGoogle Cloud Storage\n\nGoogle Cloud Storage is generally not supported for Posit application specific workloads, but can be used to store data accessed via the IDEs. Since it doesn’t present as a traditional NFS filesystem, load testing/application validation was not performed."
  },
  {
    "objectID": "storage-recommend.html",
    "href": "storage-recommend.html",
    "title": "Posit Team Cloud Storage Recommendations",
    "section": "",
    "text": "The storage solution that supports your deployment of Posit Team is critical to achieving the level of performance and reliability that your data scientists expect and need to support the critical research, development and insight generation they are striving for. The correct storage choice can ultimately save your organization time, money, and frustration by enabling a desktop-like experience for your users while providing a secure, server-sized computing environment. Because this choice can be the linchpin of a successful and satisfying deployment of Posit Workbench, Connect, and Package Manager, Posit has done extensive testing and troubleshooting with various cloud storage solutions. A combination of artificial and real-world storage loading scenarios are used in testing to try and best capture the characteristics of the underlying cloud storage solution.\n\nPosit Team Relative Storage Requirements\nThe networked and local storage needs and requirements for Posit products vary between Workbench, Connect, and Package Manager. Workbench represents the most storage-dependent product suite because it directly hosts interactive user sessions that need to be saved, cached, and run as quickly as the user’s desktop environments. Additionally, Workbench uses the user’s home directory heavily as a location for storing Workbench session state as well as suspended session data, which can grow quite large, depending on the user’s work.\nConnect’s storage requirements are generally less demanding. The difference between Workbench and Connect is that while Connect still needs fast, responsive storage, most storage-intensive operations happen asynchronously during publishing. This means that unless the application/content item does heavy read/write operations at run time, the startup and loading time for a particular application is only marginally impacted by the speed of the underlying storage.\nPackage Manager is the least storage-intensive application within Posit Team. Its primary use for storage is to locally cache packages downloaded from the Posit Sync Service and then expose those packages for consumption by your internal data science teams. This low threshold for storage performance is further evidenced by the fact that its cached package data can be served directly from blob storage services, like AWS S3.\n\n\nGeneral Storage Recommendations\nTwo components of a storage solution will generally provide an excellent user experience with Posit products, particularly in Workbench. The two primary requirements are throughput and storage latency. Storage throughput, typically measured in MB/s, measures the maximum rate at which data can be moved from server memory to disk, whether that disk is a local SSD or an NFS server share hosted in another country. Throughput is generally most utilized in data science workloads when reading or writing large data files to disk and when a user session that included large amounts of data stored in memory is suspended or resumed by Workbench. Low throughput or otherwise slow storage can massively impact code run time and the time it takes to suspend and resume user sessions. Insufficient throughput on a remote NFS server can cause cascading impacts to other users’ Workbench sessions if the network throughput of the remote NFS server where user home directories are stored is exhausted.\nThe other important metric for data science workloads is storage latency, measured in milliseconds, which represents the time a single storage transaction takes to complete. Storage latency is particularly important because session state is rapidly cached to disk to ensure that no data or inputs are lost in the event of a browser disconnect. Slowness in disk latency can cause the RStudio IDE to lag and respond slowly to user input while the IDE is trying to confirm that user input is cached.\nIOPS, while a key metric for many storage providers and solutions, is rarely the limiting factor with most cloud storage solutions because while Posit products do read and write large files and require the ability to quickly complete small file operations, most cloud storage providers provide sufficient baseline IOPS for SSD/Flash storage arrays.\nNow, on to the recommendations; generally, we recommend using this formula to calculate your required throughput needs:\nThroughput-of-1-node + (Throughput-of-1-node*Number-of-Remaining-Nodes*.1)\nFor example, if you are running HA Workbench with three nodes and each node is capable of 1 Gb/s throughput, we recommend that your storage solution can handle 1.2 Gb/s. Using the formula:\n1 Gb/s + (1 Gb/s * 2 * .1) = 1.2 Gb/s\nThis configuration generally provides sufficient throughput for a heavy workload on a single node while the remaining nodes still have the necessary throughput to handle general interactive sessions.\nOn the latency side, generally, under 1 ms, average latency measured by ioping provides a good user experience. Anything greater than 2-2.5ms might be perceived as slow performance, but depending on the other characteristics of the storage solution, it can work. Latency greater than 2.5ms for a simple ioping average causes user-perceived performance slowness and can make Workbench appear unresponsive. Examples of cloud storage solutions in this last class that administrators should avoid are Azure Files (Storage account), Azure Elastic SAN, and AWS EFS Regional.\n\n\nCloud-Specific Recommendations\n\nAWSAzureGCP\n\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\n\nStorage Service\nWorkbench Home Directory\nConnect Application Directory\nPackage Manager Local Cache\nProject Sharing Support\nFile locks\n\n\n\n\nEBS Local Storage\n✅\n✅\n✅\n✅\n✅\n\n\nAWS Fsx for Lustre\n✅\n✅\n✅\n✅\n✅\n\n\nAWS Fsx for NetApp ONTAP\n✅\n✅\n✅\n❌\n✅\n\n\nAWS Fsx for OpenZFS\n✅\n✅\n✅\n❌\n✅\n\n\nEFS One Zone\n⚠️\n✅\n✅\n❌\n✅\n\n\nEFS Regional\n⚠️\n✅\n✅\n❌\n✅\n\n\nS3 Bucket\n❌\n❌\n✅\n❌\n❌\n\n\n\n\nEBS Local Storage This standard block storage offering provides, as expected, the fastest storage experience in most Posit workloads. Unfortunately, it doesn’t support high availability or Load-Balanced configurations for Posit products. Additionally, backups and point-in-time recovery for EBS storage can result in lost data and/or work due to the generally unreplicated nature of the storage.\nAWS Fsx for Lustre\n\nFsx for Lustre provides a premium experience for data science users and supports the extended POSIX ACLs that Posit Workbench Project Sharing requires. Unfortunately, it does not include a multi-AZ redundancy option that is easy to configure. Fsx for Lustre’s replication configuration involves a potentially complicated process involving S3 bucket replication. Link S3 and Fsx for Lustre\n\nAWS Fsx for NetApp ONTAP and AWS Fsx for OpenZFS\n\nBoth filesystems provide excellent performance characteristics for most Posit Team use cases when correctly configured. They do not support Workbench project sharing because they lack support for extended Posix ACLs. With these filesystems, the key consideration is the synchronous nature of multi-AZ replication, which can introduce significant file system latency and result in perceived slowness for data science users doing simple tasks like installing R packages and creating Python virtual environments. This latency only exists for write performance because read operations can be returned directly from the primary ZFS server.\n\nEFS One Zone and EFS Regional\n\nEFS storage deployments are a relatively inexpensive solution for the Posit Team suite of products. Cost reduction is the primary benefit because they are generally slower than the other supported options and, in the case of EFS Regional, much slower. EFS Regional should not be considered for home directory storage for Workbench, though EFS One Zone is serviceable. The speed difference between EFS One Zone and EBS/Fsx solutions will directly impact user workloads and general operations in Workbench, such as installing packages and creating Python virtual environments. EFS deployments are generally performant enough for both Posit Connect and Package Manager as long as your Connect applications can be tuned to deal with the potentially extended application startup times. Package Manager can deal with slower shared filesystems and run from EFS perfectly, but generally, S3 is a better option for Package Manager.\n\n\n\n\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\n\nStorage Service\nWorkbench Home Directory\nConnect Application Directory\nPackage Manager Local Cache\nProject Sharing Support\nFile locks\n\n\n\n\nManaged Premium SSD LRS\n✅\n✅\n✅\n✅\n✅\n\n\nAzure Netapp Files - Standard\n⚠️\n⚠️\n✅\n✅\n✅\n\n\nAzure Netapp Files - Premium\n✅\n✅\n✅\n✅\n✅\n\n\nAzure Netapp Files - Ultra\n✅\n✅\n✅\n✅\n✅\n\n\nAzure Files - Storage Acct\n❌\n⚠️\n✅\n❌\n✅\n\n\nAzure Elastic SAN\n❌\n❌\n❌\n❌\n✅\n\n\n\n\nManaged Premium Disk LRS As expected, this standard block storage offering provides the fastest storage experience in most Posit workloads, but unfortunately, it doesn’t support High-Availability or Load-Balanced configurations for Posit products. Additionally, backups and point-in-time recovery for Managed Disk storage can result in lost data and/or work due to the generally unreplicated nature of the storage. Posit recommends SSD block storage for all Posit workloads, where data access speed is the most important factor in your storage selection process.\nAzure Netapp Files\n\nAzure Netapp files provide an excellent experience when used as Posit storage. Posit recommends locating the Netapp storage volume used without products in the same availability zone as your Azure VM. Azure Netapp storage throttles throughput with storage capacity and storage tier, Standard, Premium, and Ultra. Posit generally recommends against Azure Netapp- Standard for use with our products unless your organization needs or uses a lot of Azure Netapp storage because Standard doesn’t provide much throughput per TB of storage. Details can be found in the Microsoft Azure Netapp article, which you can compare with our suggested throughput formula above to determine the best capacity/throughput/cost configuration for your organization.\n\nAzure Files NFS - Storage Account\n\nAzure Files is a low-cost, scalable NFS file storage option in Azure. Still, unfortunately, it suffers from several major drawbacks when used with Posit products and data science workloads. Firstly, it has very high file system latency, resulting in a poor experience for most Posit Workbench use cases. This high latency can also result in a higher than expected occurrence of orphaned ..nfs12345678 files, which can lock/abort application workloads. These leftover files result from the NFSv3 Silly Rename process. Additionally, Azure Files suffers from known filesystem latency issues on metadata operations which further compounds its unsuitability for workloads that include lots of small files, such as R and Python package installation and restoration.\n\nAzure Elastic SAN\n\nAzure Elastic SAN showed inconsistent performance in our testing, and while it may be tunable to ultimately be an excellent storage solution, the complexity, and effort required added to the availability of more suitable, easier-to-support options like Managed Disk for local storage and Azure Netapp files for networked storage, mean that it is generally not recommended unless your organization is already using Elastic SAN and can’t use other options.\n\n\n\n\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\n\nStorage Service\nWorkbench Home Directory\nConnect Application Directory\nPackage Manager Local Cache\nProject Sharing Support\nFile locks\n\n\n\n\nSSD Persistent Disk\n✅\n✅\n✅\n✅\n✅\n\n\nGoogle File Store - Basic SSD\n✅\n✅\n✅\n✅\n✅\n\n\nGoogle File Store - Zonal SSD\n⚠️\n✅\n✅\n✅\n✅\n\n\nGoogle File Store - Enterprise SSD\n⚠️\n✅\n✅\n✅\n✅\n\n\nGoogle Cloud Storage\n❌\n❌\n❌\n❌\n❌\n\n\n\n\nSSD Persistent Disk This standard block storage offering provides, as expected, the fastest storage experience in most Posit workloads. Unfortunately, it doesn’t support High-Availability or Load-Balanced configurations for Posit products. Additionally, backups and point-in-time recovery for local disk storage can result in lost data and/or work due to the generally unreplicated nature of the storage. Posit recommends SSD block storage for all Posit workloads, where data access speed is the most important factor in your storage selection process.\nGoogle File Store\n\nGoogle File Store is one of the core NFS file server options available in GCP and thus is the primary recommended filestore for Posit products in GCP. Unfortunately, it is slower compared to other AWS and Azure cloud storage solutions, at least for single VM workloads, which was how Posit tested. Zonal and Enterprise SSDs, likely due to their HA/replicated configuration, have higher latency but higher throughput than Basic SSD. This presents shorter small file write times for Basic SSD, which manifests as shorter R and Python package installation times but longer large file installations due to lower throughput. Posit recommends sizing your GFS deployment to ensure that you can reach a good balance between throughput and latency for your users.\n\nGoogle Cloud Storage Google Cloud Storage is generally not supported for Posit application-specific workloads but can be used to store data accessed via the IDEs. Since it doesn’t present as a traditional NFS filesystem, load testing/application validation was not performed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cloud Storage Recommendations",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]